{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['Groq_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "llm = Ollama(model='llama3')\n",
    "LLM = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "Wiki_wrapper = WikipediaAPIWrapper(top_k_results=1 , doc_content_chars_max=1000)\n",
    "wiki_query= WikipediaQueryRun(api_wrapper=Wiki_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=2 , doc_content_chars_max=1000)\n",
    "arxiv_query = ArxivQueryRun(api_wrapper=arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "data_loader = WebBaseLoader(\"https://api.python.langchain.com/en/latest/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html\")\n",
    "web_data = data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_data = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 200)\n",
    "chunks = split_data.split_documents(web_data[:100])\n",
    "\n",
    "embedd = OllamaEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=chunks , embedding=embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'optimum[openvino,nncf]'\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel , Field\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompts = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineList(BaseModel):\n",
    "    Lines : list[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) ->None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "    \n",
    "    def parse(self, text: str) ->LineList:\n",
    "        line  = text.strip().split(\"\\n\")\n",
    "        return LineList(Lines=line)\n",
    "\n",
    "outputparse = LineListOutputParser()\n",
    "\n",
    "llm_chain = LLMChain(llm=LLM , prompt=prompts , output_parser=outputparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder,ChatPromptTemplate\n",
    "context_q_prompt_system =\"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "context_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system' ,context_q_prompt_system),\n",
    "        MessagesPlaceholder('Chat_history'),\n",
    "        ('human', \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "Multi_query = MultiQueryRetriever(\n",
    "    retriever = vectorstore.as_retriever() , llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "history_aware_history = create_history_aware_retriever(LLM,Multi_query,context_q_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\ \"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system' ,qa_system_prompt),\n",
    "        MessagesPlaceholder('chat_history'),\n",
    "        ('human', \"{context}\"),\n",
    "       \n",
    "])\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "stuff_documents = create_stuff_documents_chain(LLM , qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_history,stuff_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2866608970.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[71], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    query_versaion = llm_chain.run(question=question + )\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "def Answer_Multiquery(question : str, chat_history : list[str]):\n",
    "    query_versaion = llm_chain.run(question=question + )\n",
    "    print(f\"Multiple quer : {query_versaion}\")\n",
    "\n",
    "    answer = rag_chain.run(input =question , chat_history=chat_history)\n",
    "    chat_history.append(f\"User: {question}\")\n",
    "    chat_history.append(f\"Bot: {answer}\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"PromptTemplate\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mAnswer_Multiquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is OpenAI embedding model integration.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 3\u001b[0m, in \u001b[0;36mAnswer_Multiquery\u001b[1;34m(question, chat_history)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mAnswer_Multiquery\u001b[39m(question : \u001b[38;5;28mstr\u001b[39m, chat_history : \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m----> 3\u001b[0m     query_versaion \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39mrun(question\u001b[38;5;241m=\u001b[39m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple quer : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_versaion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     answer \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39mquestion , chat_history\u001b[38;5;241m=\u001b[39mchat_history)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"PromptTemplate\") to str"
     ]
    }
   ],
   "source": [
    "Answer_Multiquery('what is OpenAI embedding model integration.',chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
